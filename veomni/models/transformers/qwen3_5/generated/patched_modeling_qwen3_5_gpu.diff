--- a/transformers/models/qwen3_5/modeling_qwen3_5.py
+++ b/patched_modeling_qwen3_5_gpu.py
@@ -1,68 +1,75 @@
-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
-#           This file was automatically generated from src/transformers/models/qwen3_5/modular_qwen3_5.py.
-#               Do NOT edit this file manually as any edits will be overwritten by the generation of
-#             the file from the modular. If any change should be done, please apply the change to the
-#                          modular_qwen3_5.py file directly. One of our CI enforces this.
-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
-# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.
+# ==============================================================================
+#  AUTO-GENERATED FILE - DO NOT EDIT DIRECTLY
+# ==============================================================================
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+#  Source: transformers.models.qwen3_5.modeling_qwen3_5
+#  Based on: transformers==5.2.0
+#  Generated: 2026-02-24T22:04:19.292424
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+#  This file was generated by the modeling code generator.
+#  It contains a patched version of the original HuggingFace modeling code.
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+#  Patches applied:
+#    - method_override: Qwen3_5GatedDeltaNet.forward
+#      Support varlen flash linear attention in Qwen3_5GatedDeltaNet.forward
+#    - method_override: Qwen3_5DecoderLayer.forward
+#      Extract and pass cu_seq_lens_q for varlen linear attention in Qwen3_5DecoderLayer.forward
+#    - method_override: Qwen3_5TextModel.forward
+#      Support SP in Qwen3_5TextModel.forward
+#    - method_override: Qwen3_5ForCausalLM.forward
+#      Support fused cross entropy path in Qwen3_5ForCausalLM.forward
+#
+# ==============================================================================

 from collections.abc import Callable
 from dataclasses import dataclass
 from typing import Any, Optional
-
 import torch
 import torch.nn.functional as F
 from torch import nn
-
-from ... import initialization as init
-from ...activations import ACT2FN
-from ...cache_utils import Cache
-from ...generation import GenerationMixin
-from ...integrations import use_kernelized_func
-from ...masking_utils import create_causal_mask
-from ...modeling_flash_attention_utils import FlashAttentionKwargs
-from ...modeling_layers import GradientCheckpointingLayer
-from ...modeling_outputs import (
-    BaseModelOutputWithPast,
-    BaseModelOutputWithPooling,
-    CausalLMOutputWithPast,
-    ModelOutput,
-)
-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
-from ...processing_utils import Unpack
-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_compilable_check
-from ...utils.generic import is_flash_attention_requested, maybe_autocast, merge_with_config_defaults
-from ...utils.import_utils import is_causal_conv1d_available, is_flash_linear_attention_available
-from ...utils.output_capturing import capture_outputs
-from .configuration_qwen3_5 import Qwen3_5Config, Qwen3_5TextConfig, Qwen3_5VisionConfig
-
-
-if is_causal_conv1d_available():
-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update
-else:
-    causal_conv1d_update, causal_conv1d_fn = None, None
-
-if is_flash_linear_attention_available():
+from transformers import initialization as init
+from transformers.activations import ACT2FN
+from transformers.cache_utils import Cache
+from transformers.generation import GenerationMixin
+from transformers.integrations import use_kernelized_func
+from transformers.masking_utils import create_causal_mask
+from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
+from transformers.modeling_layers import GradientCheckpointingLayer
+from transformers.modeling_outputs import BaseModelOutputWithPast, BaseModelOutputWithPooling, CausalLMOutputWithPast, ModelOutput
+from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
+from transformers.processing_utils import Unpack
+from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_compilable_check
+from transformers.utils.generic import is_flash_attention_requested, maybe_autocast, merge_with_config_defaults
+from transformers.utils.import_utils import is_causal_conv1d_available, is_flash_linear_attention_available
+from transformers.utils.output_capturing import capture_outputs
+from transformers.models.qwen3_5.configuration_qwen3_5 import Qwen3_5Config, Qwen3_5TextConfig, Qwen3_5VisionConfig
+
+# Additional import blocks for patches
+# Modification: We are not using https://github.com/Dao-AILab/causal-conv1d now
+# we are using the triton impl of causal_conv1d from fla.
+# TODO: Evaluate Tridao's impl in the future.
+try:
     from fla.modules import FusedRMSNormGated
+    from fla.modules.convolution import causal_conv1d as causal_conv1d_fn
+    from fla.modules.convolution import causal_conv1d_update
     from fla.ops.gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule
-else:
+except ImportError:
     chunk_gated_delta_rule, fused_recurrent_gated_delta_rule = None, None
     FusedRMSNormGated = None
+    causal_conv1d_update, causal_conv1d_fn = None, None
+    logging.get_logger(__name__).warning(
+        "Failed to import FLA modules: fallback to eager implementation. "
+        "This case can't support rmpad_with_pos_ids=True!"
+    )
+
+# Additional imports for patches
+from veomni.distributed.parallel_state import get_parallel_state
+from veomni.distributed.sequence_parallel import slice_position_embedding
+

 logger = logging.get_logger(__name__)
+


 class Qwen3_5DynamicCache:
@@ -155,6 +162,7 @@
         return self.conv_states[self.last_linear_layer] is not None


+
 class Qwen3_5VisionRotaryEmbedding(nn.Module):
     inv_freq: torch.Tensor  # fix linting for `register_buffer`

@@ -169,6 +177,7 @@
         seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
         freqs = torch.outer(seq, self.inv_freq)
         return freqs
+


 class Qwen3_5TextRotaryEmbedding(nn.Module):
@@ -261,6 +270,7 @@
         return freqs_t


+
 class Qwen3_5RMSNormGated(nn.Module):
     def __init__(self, hidden_size, eps=1e-6, **kwargs):
         super().__init__()
@@ -279,6 +289,7 @@
         return hidden_states.to(input_dtype)


+
 def apply_mask_to_padding_states(hidden_states, attention_mask):
     """
     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66
@@ -291,9 +302,11 @@
     return hidden_states


+
 is_fast_path_available = all(
     (causal_conv1d_fn, causal_conv1d_update, chunk_gated_delta_rule, fused_recurrent_gated_delta_rule)
 )
+


 def torch_causal_conv1d_update(
@@ -314,10 +327,12 @@
     return out


+
 def l2norm(x: torch.FloatTensor, dim: int = -1, eps: float = 1e-6):
     """This function is intended to align with the l2norm implementation in the FLA library."""
     inv_norm = torch.rsqrt((x * x).sum(dim=dim, keepdim=True) + eps)
     return x * inv_norm
+


 def torch_chunk_gated_delta_rule(
@@ -400,6 +415,7 @@
     return core_attn_out, last_recurrent_state


+
 def torch_recurrent_gated_delta_rule(
     query, key, value, g, beta, initial_state, output_final_state, use_qk_l2norm_in_kernel=False
 ):
@@ -442,6 +458,12 @@
     return core_attn_out, last_recurrent_state


+# ======================================================================
+# [MODIFIED CLASS] Qwen3_5GatedDeltaNet
+# Methods patched: forward
+# ======================================================================
+
+
 class Qwen3_5GatedDeltaNet(nn.Module):
     def __init__(self, config: Qwen3_5Config, layer_idx: int):
         super().__init__()
@@ -514,6 +536,8 @@
         cache_params: Qwen3_5DynamicCache | None = None,
         cache_position: torch.LongTensor | None = None,
         attention_mask: torch.Tensor | None = None,
+        # Modification: plumb varlen sequence metadata to FLA kernels.
+        cu_seq_lens_q: torch.Tensor | None = None,
     ):
         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)

@@ -544,27 +568,26 @@
         if use_precomputed_states:
             # 2. Convolution sequence transformation
             # NOTE: the conv state is updated in `causal_conv1d_update`
-            mixed_qkv = self.causal_conv1d_update(
-                mixed_qkv,
-                conv_state,
-                self.conv1d.weight.squeeze(1),
-                self.conv1d.bias,
-                self.activation,
-            )
+            # Modification: keep this disabled until FLA causal_conv1d_update decode path is validated.
+            raise NotImplementedError("use_precomputed_states=True is not supported yet for causal_conv1d_update now.")
         else:
             if cache_params is not None:
                 conv_state = F.pad(mixed_qkv, (self.conv_kernel_size - mixed_qkv.shape[-1], 0))
                 cache_params.conv_states[self.layer_idx] = conv_state
             if self.causal_conv1d_fn is not None:
+                # Modification: FLA causal_conv1d expects [B, S, D], while upstream tensor is [B, D, S].
                 mixed_qkv = self.causal_conv1d_fn(
-                    x=mixed_qkv,
+                    x=mixed_qkv.transpose(1, 2),
                     weight=self.conv1d.weight.squeeze(1),
                     bias=self.conv1d.bias,
                     activation=self.activation,
                     seq_idx=None,
-                )
+                    backend="triton",
+                    # Modification: pass varlen boundaries to FLA conv kernel.
+                    cu_seqlens=cu_seq_lens_q,
+                )[0].transpose(1, 2)
             else:
-                mixed_qkv = F.silu(self.conv1d(mixed_qkv)[:, :, :seq_len])
+                raise NotImplementedError("This path is not supported yet because it can't process varlen now.")

         mixed_qkv = mixed_qkv.transpose(1, 2)
         query, key, value = torch.split(
@@ -584,22 +607,30 @@
         beta = b.sigmoid()
         # If the model is loaded in fp16, without the .float() here, A might be -inf
         g = -self.A_log.float().exp() * F.softplus(a.float() + self.dt_bias)
+
         if self.num_v_heads // self.num_k_heads > 1:
             query = query.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)
             key = key.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)

         if not use_precomputed_states:
-            core_attn_out, last_recurrent_state = self.chunk_gated_delta_rule(
-                query,
-                key,
-                value,
-                g=g,
-                beta=beta,
-                initial_state=None,
-                output_final_state=cache_params is not None,
-                use_qk_l2norm_in_kernel=True,
-            )
-
+            if self.chunk_gated_delta_rule is torch_chunk_gated_delta_rule:
+                raise RuntimeError(
+                    "Varlen training requires FLA. Install flash-linear-attention so "
+                    "chunk_gated_delta_rule supports cu_seqlens."
+                )
+            else:
+                # Modification: use direct args and pass cu_seqlens for varlen FLA attention.
+                core_attn_out, last_recurrent_state = self.chunk_gated_delta_rule(
+                    query,
+                    key,
+                    value,
+                    g=g,
+                    beta=beta,
+                    initial_state=None,
+                    output_final_state=cache_params is not None,
+                    use_qk_l2norm_in_kernel=True,
+                    cu_seqlens=cu_seq_lens_q,
+                )
         else:
             core_attn_out, last_recurrent_state = self.recurrent_gated_delta_rule(
                 query,
@@ -626,11 +657,13 @@
         return output


+
 def rotate_half(x):
     """Rotates half the hidden dims of the input."""
     x1 = x[..., : x.shape[-1] // 2]
     x2 = x[..., x.shape[-1] // 2 :]
     return torch.cat((-x2, x1), dim=-1)
+


 # Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb
@@ -672,6 +705,7 @@
     return q_embed, k_embed


+
 def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
     """
     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
@@ -682,6 +716,7 @@
         return hidden_states
     hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
+


 def eager_attention_forward(
@@ -709,6 +744,7 @@
     return attn_output, attn_weights


+
 @use_kernelized_func(apply_rotary_pos_emb)
 class Qwen3_5Attention(nn.Module):
     """Multi-headed attention from 'Attention Is All You Need' paper"""
@@ -788,6 +824,7 @@
         return attn_output, attn_weights


+
 class Qwen3_5MLP(nn.Module):
     def __init__(self, config: Qwen3_5Config, intermediate_size: int):
         super().__init__()
@@ -804,6 +841,7 @@
         return down_proj


+
 class Qwen3_5RMSNorm(nn.Module):
     def __init__(self, dim: int, eps: float = 1e-6):
         super().__init__()
@@ -822,6 +860,12 @@

     def extra_repr(self):
         return f"{tuple(self.weight.shape)}, eps={self.eps}"
+
+
+# ======================================================================
+# [MODIFIED CLASS] Qwen3_5DecoderLayer
+# Methods patched: forward
+# ======================================================================


 class Qwen3_5DecoderLayer(GradientCheckpointingLayer):
@@ -851,13 +895,22 @@

         hidden_states = self.input_layernorm(hidden_states)

+        # Modification: read varlen metadata from kwargs and enforce it for linear-attention varlen kernels.
+        cu_seq_lens_q = kwargs.get("cu_seq_lens_q", None)
+        assert cu_seq_lens_q is not None, (
+            "cu_seq_lens_q must be provided to support varlen Flash Linear Attention, varlen Conv1D,"
+            "and to remove the full Flash Attention CPU-GPU sync."
+        )
+
         # Token Mixer
         if self.layer_type == "linear_attention":
+            # Modification: pass cu_seq_lens_q through to Qwen3_5GatedDeltaNet.forward.
             hidden_states = self.linear_attn(
                 hidden_states=hidden_states,
                 cache_params=past_key_values,
                 cache_position=cache_position,
                 attention_mask=attention_mask,
+                cu_seq_lens_q=cu_seq_lens_q,
             )
         elif self.layer_type == "full_attention":
             # Self Attention
@@ -878,8 +931,8 @@
         hidden_states = self.post_attention_layernorm(hidden_states)
         hidden_states = self.mlp(hidden_states)
         hidden_states = residual + hidden_states
-
         return hidden_states
+


 class Qwen3_5PreTrainedModel(PreTrainedModel):
@@ -911,6 +964,7 @@
             init.copy_(module.inv_freq, inv_freq)


+
 class Qwen3_5VisionMLP(nn.Module):
     def __init__(self, config):
         super().__init__()
@@ -924,6 +978,7 @@
         return self.linear_fc2(self.act_fn(self.linear_fc1(hidden_state)))


+
 class Qwen3_5VisionPatchEmbed(nn.Module):
     def __init__(self, config) -> None:
         super().__init__()
@@ -942,6 +997,7 @@
         )
         hidden_states = self.proj(hidden_states.to(dtype=target_dtype)).view(-1, self.embed_dim)
         return hidden_states
+


 class Qwen3_5VisionPatchMerger(nn.Module):
@@ -958,6 +1014,7 @@
         x = self.norm(x.view(-1, self.hidden_size) if self.use_postshuffle_norm else x).view(-1, self.hidden_size)
         x = self.linear_fc2(self.act_fn(self.linear_fc1(x)))
         return x
+


 def apply_rotary_pos_emb_vision(
@@ -972,6 +1029,7 @@
     q_embed = q_embed.to(orig_q_dtype)
     k_embed = k_embed.to(orig_k_dtype)
     return q_embed, k_embed
+


 class Qwen3_5VisionAttention(nn.Module):
@@ -1057,6 +1115,7 @@
         return attn_output


+
 class Qwen3_5VisionBlock(GradientCheckpointingLayer):
     def __init__(self, config, attn_implementation: str = "sdpa") -> None:
         super().__init__()
@@ -1084,6 +1143,7 @@
         return hidden_states


+
 class Qwen3_5VisionModel(Qwen3_5PreTrainedModel):
     config: Qwen3_5VisionConfig
     _no_split_modules = ["Qwen3_5VisionBlock"]
@@ -1273,6 +1333,7 @@
         )


+
 @dataclass
 @auto_docstring(
     custom_intro="""
@@ -1295,6 +1356,12 @@
     hidden_states: tuple[torch.FloatTensor] | None = None
     attentions: tuple[torch.FloatTensor] | None = None
     rope_deltas: torch.LongTensor | None = None
+
+
+# ======================================================================
+# [MODIFIED CLASS] Qwen3_5TextModel
+# Methods patched: forward
+# ======================================================================


 class Qwen3_5TextModel(Qwen3_5PreTrainedModel):
@@ -1330,6 +1397,10 @@
         if inputs_embeds is None:
             inputs_embeds = self.embed_tokens(input_ids)

+        if self.gradient_checkpointing and self.training and use_cache:
+            logger.warning_once("`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.")
+            use_cache = False
+
         if use_cache and past_key_values is None:
             past_key_values = Qwen3_5DynamicCache(config=self.config)

@@ -1364,7 +1435,12 @@
         hidden_states = inputs_embeds
         position_embeddings = self.rotary_emb(hidden_states, position_ids)

-        for layer_idx, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):
+        # ============================== VeOmni SP Patch Start ==============================
+        sp_group = get_parallel_state().sp_group if get_parallel_state().sp_enabled else None
+        position_embeddings = slice_position_embedding(position_embeddings, dim=1, sp_group=sp_group)
+        # =============================== VeOmni SP Patch End ===============================
+
+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
             layer_mask = linear_attn_mask if decoder_layer.layer_type == "linear_attention" else causal_mask

             hidden_states = decoder_layer(
@@ -1396,6 +1472,7 @@
         if cache_position[0] > 0 or (attention_mask is not None and torch.all(attention_mask == 1)):
             linear_attn_mask = None
         return linear_attn_mask
+


 @auto_docstring
@@ -1707,6 +1784,12 @@
         )


+# ======================================================================
+# [MODIFIED CLASS] Qwen3_5ForCausalLM
+# Methods patched: forward
+# ======================================================================
+
+
 @auto_docstring
 class Qwen3_5ForCausalLM(Qwen3_5PreTrainedModel, GenerationMixin):
     _tied_weights_keys = {"lm_head.weight": "model.embed_tokens.weight"}
@@ -1739,28 +1822,6 @@
         logits_to_keep: int | torch.Tensor = 0,
         **kwargs: Unpack[TransformersKwargs],
     ) -> CausalLMOutputWithPast:
-        r"""
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-        Example:
-
-        ```python
-        >>> from transformers import AutoTokenizer, Qwen3_5ForCausalLM
-
-        >>> model = Qwen3_5ForCausalLM.from_pretrained("Qwen/Qwen3_5-8B")
-        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3_5-8B")
-
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-        ```"""
         outputs: BaseModelOutputWithPast = self.model(
             input_ids=input_ids,
             attention_mask=attention_mask,
@@ -1775,11 +1836,21 @@
         hidden_states = outputs.last_hidden_state
         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
-        logits = self.lm_head(hidden_states[:, slice_indices, :])
+        hidden_states = hidden_states[:, slice_indices, :]

         loss = None
+        logits = None
         if labels is not None:
-            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
+            loss, logits = self.loss_function(
+                logits=logits,
+                labels=labels,
+                vocab_size=self.config.vocab_size,
+                hidden_states=hidden_states,
+                weights=self.lm_head.weight,
+                **kwargs,
+            )
+        else:
+            logits = self.lm_head(hidden_states)

         return CausalLMOutputWithPast(
             loss=loss,
@@ -1788,6 +1859,7 @@
             hidden_states=outputs.hidden_states,
             attentions=outputs.attentions,
         )
+


 @dataclass
@@ -1817,6 +1889,7 @@
     hidden_states: tuple[torch.FloatTensor] | None = None
     attentions: tuple[torch.FloatTensor] | None = None
     rope_deltas: torch.LongTensor | None = None
+


 class Qwen3_5ForConditionalGeneration(Qwen3_5PreTrainedModel, GenerationMixin):
@@ -2193,6 +2266,7 @@
         return input_ids, model_kwargs


+
 __all__ = [
     "Qwen3_5VisionModel",
     "Qwen3_5TextModel",
