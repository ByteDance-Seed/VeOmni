--- a/transformers/models/qwen3_moe/modeling_qwen3_moe.py
+++ b/patched_modeling_qwen3_moe_gpu.py
@@ -1,56 +1,59 @@
-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
-#           This file was automatically generated from src/transformers/models/qwen3_moe/modular_qwen3_moe.py.
-#               Do NOT edit this file manually as any edits will be overwritten by the generation of
-#             the file from the modular. If any change should be done, please apply the change to the
-#                          modular_qwen3_moe.py file directly. One of our CI enforces this.
-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
-# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
+# ==============================================================================
+#  AUTO-GENERATED FILE - DO NOT EDIT DIRECTLY
+# ==============================================================================
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+#  Source: transformers.models.qwen3_moe.modeling_qwen3_moe
+#  Based on: transformers==5.2.0
+#  Generated: 2026-02-23T03:27:25.022789
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+#  This file was generated by the modeling code generator.
+#  It contains a patched version of the original HuggingFace modeling code.
 #
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+#  Patches applied:
+#    - class_replacement: Qwen3MoeRMSNorm
+#      Use LigerKernel RMSNorm
+#    - class_replacement: Qwen3MoeMLP
+#      Use LigerKernel SwiGLU MLP
+#    - class_replacement: Qwen3MoeExperts
+#      Use legacy gate/up/down expert weights and explicit VeOmni fused MoE path
+#    - function_replacement: apply_rotary_pos_emb
+#      Use LigerKernel rotary embedding
+#    - method_override: Qwen3MoeModel.forward
+#      Support SP in Qwen3MoeModel.forward
+#    - method_override: Qwen3MoeForCausalLM.forward
+#      Support fused cross entropy path in Qwen3MoeForCausalLM.forward
+#    - method_override: Qwen3MoeForCausalLM.get_parallel_plan
+#      Register Qwen3Moe expert parallel plan for v5 generated modeling
+#
+# ==============================================================================

 from collections.abc import Callable
 from typing import Optional
-
 import torch
 import torch.nn.functional as F
 from torch import nn
-
-from ... import initialization as init
-from ...activations import ACT2FN
-from ...cache_utils import Cache, DynamicCache
-from ...generation import GenerationMixin
-from ...integrations import (
-    use_experts_implementation,
-    use_kernel_forward_from_hub,
-    use_kernel_func_from_hub,
-    use_kernelized_func,
-)
-from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask
-from ...modeling_flash_attention_utils import FlashAttentionKwargs
-from ...modeling_layers import (
-    GenericForQuestionAnswering,
-    GenericForSequenceClassification,
-    GenericForTokenClassification,
-    GradientCheckpointingLayer,
-)
-from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast
-from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
-from ...processing_utils import Unpack
-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available
-from ...utils.generic import maybe_autocast, merge_with_config_defaults
-from ...utils.output_capturing import OutputRecorder, capture_outputs
-from .configuration_qwen3_moe import Qwen3MoeConfig
+from transformers import initialization as init
+from transformers.activations import ACT2FN
+from transformers.cache_utils import Cache, DynamicCache
+from transformers.generation import GenerationMixin
+from transformers.integrations import use_experts_implementation, use_kernel_forward_from_hub, use_kernel_func_from_hub, use_kernelized_func
+from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
+from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
+from transformers.modeling_layers import GenericForQuestionAnswering, GenericForSequenceClassification, GenericForTokenClassification, GradientCheckpointingLayer
+from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast
+from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
+from transformers.processing_utils import Unpack
+from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple, is_grouped_mm_available
+from transformers.utils.generic import maybe_autocast, merge_with_config_defaults
+from transformers.utils.output_capturing import OutputRecorder, capture_outputs
+from transformers.models.qwen3_moe.configuration_qwen3_moe import Qwen3MoeConfig
+
+# Additional imports for patches
+from veomni.distributed.parallel_state import get_parallel_state
+from veomni.distributed.sequence_parallel import slice_position_embedding
+from veomni.ops import fused_moe_forward
+


 def rotate_half(x):
@@ -60,30 +63,30 @@
     return torch.cat((-x2, x1), dim=-1)


-@use_kernel_func_from_hub("rotary_pos_emb")
-def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):
-    """Applies Rotary Position Embedding to the query and key tensors.
-
-    Args:
-        q (`torch.Tensor`): The query tensor.
-        k (`torch.Tensor`): The key tensor.
-        cos (`torch.Tensor`): The cosine part of the rotary embedding.
-        sin (`torch.Tensor`): The sine part of the rotary embedding.
-        unsqueeze_dim (`int`, *optional*, defaults to 1):
-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
-    Returns:
-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
-    """
-    cos = cos.unsqueeze(unsqueeze_dim)
-    sin = sin.unsqueeze(unsqueeze_dim)
-    q_embed = (q * cos) + (rotate_half(q) * sin)
-    k_embed = (k * cos) + (rotate_half(k) * sin)
-    return q_embed, k_embed
+# ======================================================================
+# [PATCHED FUNCTION] apply_rotary_pos_emb
+# Reason: Use LigerKernel rotary embedding
+# Source: veomni.models.transformers.qwen3_moe.qwen3_moe_gpu_patch_gen_config
+# ======================================================================
+def apply_rotary_pos_emb(
+    q: torch.Tensor,
+    k: torch.Tensor,
+    cos: torch.Tensor,
+    sin: torch.Tensor,
+    position_ids: Optional[torch.Tensor] = None,
+    unsqueeze_dim: int = 1,
+) -> tuple[torch.Tensor, torch.Tensor]:
+    from liger_kernel.transformers.rope import liger_rotary_pos_emb
+
+    return liger_rotary_pos_emb(
+        q,
+        k,
+        cos,
+        sin,
+        position_ids=position_ids,
+        unsqueeze_dim=unsqueeze_dim,
+    )
+


 def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
@@ -96,6 +99,7 @@
         return hidden_states
     hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
+


 def eager_attention_forward(
@@ -123,6 +127,7 @@
     return attn_output, attn_weights


+
 @use_kernelized_func(apply_rotary_pos_emb)
 class Qwen3MoeAttention(nn.Module):
     """Multi-headed attention from 'Attention Is All You Need' paper"""
@@ -198,34 +203,33 @@
         return attn_output, attn_weights


-class Qwen3MoeMLP(nn.Module):
-    def __init__(self, config, intermediate_size=None):
-        super().__init__()
-        self.config = config
-        self.hidden_size = config.hidden_size
-        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size
-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
-        self.act_fn = ACT2FN[config.hidden_act]
-
-    def forward(self, x):
-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
-        return down_proj
-
-
-@use_experts_implementation
-class Qwen3MoeExperts(nn.Module):
-    """Collection of expert weights stored as 3D tensors."""
-
+# ======================================================================
+# [PATCHED CLASS] Qwen3MoeMLP
+# Original class replaced with: external
+# Reason: Use LigerKernel SwiGLU MLP
+# Source: liger_kernel.transformers.swiglu
+# ======================================================================
+# Import from: liger_kernel.transformers.swiglu.LigerSwiGLUMLP
+from liger_kernel.transformers.swiglu import LigerSwiGLUMLP as Qwen3MoeMLP
+
+
+# ======================================================================
+# [PATCHED CLASS] Qwen3MoeExperts
+# Original class replaced with: PatchedQwen3MoeExperts
+# Reason: Use legacy gate/up/down expert weights and explicit VeOmni fused MoE path
+# Source: veomni.models.transformers.qwen3_moe.qwen3_moe_gpu_patch_gen_config
+# ======================================================================
+class Qwen3MoeExperts(torch.nn.Module):
     def __init__(self, config):
         super().__init__()
         self.num_experts = config.num_experts
         self.hidden_dim = config.hidden_size
         self.intermediate_dim = config.moe_intermediate_size
-        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))
-        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))
+        self.gate_proj = torch.nn.Parameter(torch.empty(self.num_experts, self.intermediate_dim, self.hidden_dim))
+        self.up_proj = torch.nn.Parameter(torch.empty(self.num_experts, self.intermediate_dim, self.hidden_dim))
+        self.down_proj = torch.nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))
         self.act_fn = ACT2FN[config.hidden_act]
+        self._moe_implementation = getattr(config, "_moe_implementation", "eager")

     def forward(
         self,
@@ -234,24 +238,40 @@
         top_k_weights: torch.Tensor,
     ) -> torch.Tensor:
         final_hidden_states = torch.zeros_like(hidden_states)
-        with torch.no_grad():
-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)
-            expert_mask = expert_mask.permute(2, 1, 0)
-            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()
-
-        for expert_idx in expert_hit:
-            expert_idx = expert_idx[0]
-            if expert_idx == self.num_experts:
-                continue
-            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])
-            current_state = hidden_states[token_idx]
-            gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)
-            current_hidden_states = self.act_fn(gate) * up
-            current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])
-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]
-            final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))
+        if self._moe_implementation == "eager":
+            with torch.no_grad():
+                expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)
+                expert_mask = expert_mask.permute(2, 1, 0)
+                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()
+
+            for expert_idx in expert_hit:
+                expert_idx = expert_idx[0]
+                if expert_idx == self.num_experts:
+                    continue
+                top_k_pos, token_idx = torch.where(expert_mask[expert_idx])
+                current_state = hidden_states[token_idx]
+                gate = torch.nn.functional.linear(current_state, self.gate_proj[expert_idx])
+                up = torch.nn.functional.linear(current_state, self.up_proj[expert_idx])
+                current_hidden_states = self.act_fn(gate) * up
+                current_hidden_states = torch.nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])
+                current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]
+                final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))
+        elif self._moe_implementation == "fused":
+            final_hidden_states = fused_moe_forward(
+                module=self,
+                num_experts=self.num_experts,
+                routing_weights=top_k_weights.to(final_hidden_states.dtype),
+                selected_experts=top_k_index,
+                hidden_states=hidden_states,
+                fc1_1_weight=self.gate_proj,
+                fc1_2_weight=self.up_proj,
+                fc2_weight=self.down_proj,
+            )
+        else:
+            raise ValueError(f"Invalid moe implementation: {self._moe_implementation}")

         return final_hidden_states
+


 class Qwen3MoeTopKRouter(nn.Module):
@@ -275,6 +295,7 @@
         return router_logits, router_scores, router_indices


+
 class Qwen3MoeSparseMoeBlock(nn.Module):
     def __init__(self, config: Qwen3MoeConfig):
         super().__init__()
@@ -289,25 +310,15 @@
         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)


-@use_kernel_forward_from_hub("RMSNorm")
-class Qwen3MoeRMSNorm(nn.Module):
-    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
-        """
-        Qwen3MoeRMSNorm is equivalent to T5LayerNorm
-        """
-        super().__init__()
-        self.weight = nn.Parameter(torch.ones(hidden_size))
-        self.variance_epsilon = eps
-
-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-        input_dtype = hidden_states.dtype
-        hidden_states = hidden_states.to(torch.float32)
-        variance = hidden_states.pow(2).mean(-1, keepdim=True)
-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
-        return self.weight * hidden_states.to(input_dtype)
-
-    def extra_repr(self):
-        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"
+# ======================================================================
+# [PATCHED CLASS] Qwen3MoeRMSNorm
+# Original class replaced with: external
+# Reason: Use LigerKernel RMSNorm
+# Source: liger_kernel.transformers.rms_norm
+# ======================================================================
+# Import from: liger_kernel.transformers.rms_norm.LigerRMSNorm
+from liger_kernel.transformers.rms_norm import LigerRMSNorm as Qwen3MoeRMSNorm
+


 class Qwen3MoeDecoderLayer(GradientCheckpointingLayer):
@@ -358,6 +369,7 @@
         return hidden_states


+
 @auto_docstring
 class Qwen3MoePreTrainedModel(PreTrainedModel):
     config: Qwen3MoeConfig
@@ -387,6 +399,7 @@
             init.normal_(module.down_proj, mean=0.0, std=std)
         elif isinstance(module, Qwen3MoeTopKRouter):
             init.normal_(module.weight, mean=0.0, std=std)
+


 class Qwen3MoeRotaryEmbedding(nn.Module):
@@ -454,6 +467,12 @@
         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


+# ======================================================================
+# [MODIFIED CLASS] Qwen3MoeModel
+# Methods patched: forward
+# ======================================================================
+
+
 @auto_docstring
 class Qwen3MoeModel(Qwen3MoePreTrainedModel):
     def __init__(self, config: Qwen3MoeConfig):
@@ -500,6 +519,7 @@
             cache_position = torch.arange(
                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
             )
+
         if position_ids is None:
             position_ids = cache_position.unsqueeze(0)

@@ -515,6 +535,11 @@

         hidden_states = inputs_embeds
         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)
+
+        # ============================== VeOmni SP Patch Start ==============================
+        sp_group = get_parallel_state().sp_group if get_parallel_state().sp_enabled else None
+        position_embeddings = slice_position_embedding(position_embeddings, dim=1, sp_group=sp_group)
+        # =============================== VeOmni SP Patch End ===============================

         for decoder_layer in self.layers[: self.config.num_hidden_layers]:
             hidden_states = decoder_layer(
@@ -534,6 +559,7 @@
             last_hidden_state=hidden_states,
             past_key_values=past_key_values,
         )
+


 def load_balancing_loss_func(
@@ -616,6 +642,12 @@

     overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))
     return overall_loss * num_experts
+
+
+# ======================================================================
+# [MODIFIED CLASS] Qwen3MoeForCausalLM
+# Methods patched: forward, get_parallel_plan
+# ======================================================================


 @auto_docstring
@@ -652,34 +684,10 @@
         logits_to_keep: int | torch.Tensor = 0,
         **kwargs: Unpack[TransformersKwargs],
     ) -> MoeCausalLMOutputWithPast:
-        r"""
-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
-
-        Example:
-
-        ```python
-        >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM
-
-        >>> model = Qwen3MoeForCausalLM.from_pretrained("Qwen/Qwen3-MoE-15B-A2B")
-        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-MoE-15B-A2B")
-
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
-
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-        ```"""
-
         output_router_logits = (
             output_router_logits if output_router_logits is not None else self.config.output_router_logits
         )

-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
         outputs: MoeModelOutputWithPast = self.model(
             input_ids=input_ids,
             attention_mask=attention_mask,
@@ -693,13 +701,21 @@
         )

         hidden_states = outputs.last_hidden_state
-        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
-        logits = self.lm_head(hidden_states[:, slice_indices, :])

         loss = None
+        logits = None
         if labels is not None:
-            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)
+            loss, logits = self.loss_function(
+                logits=logits,
+                labels=labels,
+                vocab_size=self.config.vocab_size,
+                hidden_states=hidden_states,
+                weights=self.lm_head.weight,
+                **kwargs,
+            )
+        else:
+            logits = self.lm_head(hidden_states[:, slice_indices, :])

         aux_loss = None
         if output_router_logits:
@@ -710,7 +726,7 @@
                 attention_mask,
             )
             if labels is not None:
-                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device
+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)

         return MoeCausalLMOutputWithPast(
             loss=loss,
@@ -722,17 +738,26 @@
             router_logits=outputs.router_logits,
         )

+    def get_parallel_plan(self):
+        from ..parallel_plan import get_parallel_plan as _get_parallel_plan
+
+        return _get_parallel_plan()
+
+

 class Qwen3MoeForSequenceClassification(GenericForSequenceClassification, Qwen3MoePreTrainedModel):
     pass


+
 class Qwen3MoeForTokenClassification(GenericForTokenClassification, Qwen3MoePreTrainedModel):
     pass


+
 class Qwen3MoeForQuestionAnswering(GenericForQuestionAnswering, Qwen3MoePreTrainedModel):
     base_model_prefix = "transformer"  # For BC, where `transformer` was used instead of `model`
+


 __all__ = [
