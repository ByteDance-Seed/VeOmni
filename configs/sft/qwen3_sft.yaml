model:
  model_path: Qwen/Qwen3-32B
  attn_implementation: flash_attention_2

data:
  train_path: None
  data_type: conversation
  datasets_type: iterable
  dataloader_type: native
  chat_template: default
  max_seq_len: 2048
  train_size: 40000000
  text_keys: messages

train:
  num_train_epochs: 2
  max_steps: 2000
  use_wandb: false
  output_dir: qwen3_sft
  data_parallel_mode: fsdp2
  ulysses_parallel_size: 1
  expert_parallel_size: 1
  global_batch_size: 16
  micro_batch_size: 1
  rmpad: false
  rmpad_with_pos_ids: true
  bsz_warmup_ratio: 0.007
  optimizer: adamw
  lr: 1.0e-5
  lr_warmup_ratio: 0.007
  lr_decay_style: constant
  lr_decay_ratio: 1.0
  weight_decay: 0.01
  max_grad_norm: 1.0
  enable_mixed_precision: true
  enable_gradient_checkpointing: true
  enable_full_shard: true
  enable_fsdp_offload: false
  enable_activation_offload: false
  init_device: meta
  enable_full_determinism: false
  empty_cache_steps: 500
  ckpt_manager: dcp
  load_checkpoint_path: ""
  save_steps: 2000
  save_epochs: 2
  save_hf_weights: true
  wandb_project: Qwen3_32B_sft
  wandb_name: Qwen3_32B_fsdp2